{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import inspect, os, sys, copy, pytz, re, glob\n",
    "import simplejson as json\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt   # Matplotlib for plotting\n",
    "import matplotlib.dates as md\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import re, urllib\n",
    "\n",
    "utc=pytz.UTC\n",
    "\n",
    "ENV = \"production\"\n",
    "os.environ['CS_ENV'] = 'production'\n",
    "BASE_DIR = \"/home/reddit/CivilServant\"\n",
    "sys.path.append(BASE_DIR)\n",
    "subreddit_id = \"2qh13\"\n",
    "\n",
    "with open(os.path.join(BASE_DIR, \"config\") + \"/{env}.json\".format(env=ENV), \"r\") as config:\n",
    "  DBCONFIG = json.loads(config.read())\n",
    "\n",
    "### LOAD SQLALCHEMY\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import text, and_, or_\n",
    "from app.models import Base, SubredditPage, FrontPage, Subreddit, Post, ModAction, Experiment\n",
    "from utils.common import PageType\n",
    "\n",
    "db_engine = create_engine(\"mysql://{user}:{password}@{host}/{database}\".format(\n",
    "    host = DBCONFIG['host'],\n",
    "    user = DBCONFIG['user'],\n",
    "    password = DBCONFIG['password'],\n",
    "    database = DBCONFIG['database']))\n",
    "DBSession = sessionmaker(bind=db_engine)\n",
    "db_session = DBSession()\n",
    "\n",
    "### LOAD PRAW\n",
    "import reddit.connection\n",
    "conn = reddit.connection.Connect(base_dir=BASE_DIR, env=\"jupyter\")\n",
    "\n",
    "### FILTER OUT DEPRECATION WARNINGS ASSOCIATED WITH DECORATORS\n",
    "# https://github.com/ipython/ipython/issues/9242\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning, message='.*use @default decorator instead.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals of this notebook\n",
    "The goal of this notebook is to conduct an analysis of the r/worldnews experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### CONNECT TO PRAW\n",
    "#r = conn.connect(controller=\"ModLog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset of Posts from Worldnews in the Observation Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tabloid_domains = [ 'dailymail.co.uk', 'express.co.uk','mirror.co.uk', 'news.com.au', \n",
    "                    'nypost.com', 'thesun.co.uk','dailystar.co.uk','metro.co.uk']\n",
    "msm_domains     = [ \"bbc.com\", \"reuters.com\", \"nytimes.com\", \"washingtonpost.com\", \"cnn.com\", \n",
    "                    \"telegraph.co.uk\", \"latimes.com\", \"huffingtonpost.com\", \"theguardian.com\", \"forbes.com\",\n",
    "                    \"examiner.com\", \"usatoday.com\", \"wsj.com\", \"cbsnews.com\", \"cbc.ca\", \"time.com\",\n",
    "                    \"sfgate.com\", \"newsweek.com\", \"bostonglobe.com\", \"nydailynews.com\", \"msnbc.com\",\n",
    "                    \"foxnews.com\", \"aljazeera.com\", \"nbcnews.com\", \"npr.org\", \"bloomberg.com\", \"abcnews.com\", \n",
    "                    \"aljazeera.com\", \"bigstory.ap.com\", \"cbc.ca\", \"time.com\"]\n",
    "\n",
    "omitted_matches = [\"reddit\", \"img\", \"image\", \"giphy\", \"quickmeme\"]\n",
    "\n",
    "bots = ['autotldr', 'Mentioned_Videos', 'DailMail_Bot', 'youtubefactsbot', 'HelperBot_']\n",
    "\n",
    "all_posts = {}\n",
    "query_text = \"\"\"\n",
    "select * from experiment_things \n",
    "    JOIN posts ON experiment_things.id = posts.id \n",
    "    WHERE experiment_id=8 \n",
    "    AND object_type=1 \n",
    "    AND posts.created_at <= DATE_SUB( NOW() , INTERVAL 1 DAY )\n",
    "    AND posts.created_at <= '2017-01-20 13:18:22'\n",
    "    ORDER BY posts.created ASC;\n",
    "\"\"\"\n",
    "\n",
    "## ADDED THE SECOND CREATED_AT TO PRESERVE EXPERIMENT INTEGRITY\n",
    "\n",
    "for row in db_engine.execute(text(query_text)):\n",
    "    post = {}\n",
    "    post_data = json.loads(row['post_data'])\n",
    "    for key in post_data.keys():\n",
    "        post[key] = post_data[key]\n",
    "    for key in row.keys():\n",
    "        post[key]=row[key]\n",
    "    del post['post_data']\n",
    "    post['visible'] = True\n",
    "    all_posts[post['id']] = post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(len(snapshotted_posts))\n",
    "#print(len(all_posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# experiment = {}\n",
    "# for row in db_engine.execute(text(\"select name, created_at, id FROM experiments WHERE name='sticky_comment_multiarm_worldnews_0';\")):\n",
    "#     for key in row.keys():\n",
    "#         experiment[key] = row[key]\n",
    "# print(\"Earliest Date: {0}\".format(all_posts[0]['created_at']))\n",
    "earliest_date = min([x['created_at'] for x in list(all_posts.values())])\n",
    "latest_date = max([x['created_at'] for x in list(all_posts.values())])\n",
    "ranking_latest_date = max([x['created_at'] for x in list(all_posts.values())]) + datetime.timedelta(days=1)\n",
    "\n",
    "# print(\"Experiment Date: {0}\".format(experiment['created_at']))\n",
    "# print(\"Latest Date: {0}\".format(all_posts[-1]['created_at']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2017, 1, 20, 13, 18, 22)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts: 843\n",
      "13375.683333333332 minutes elapsed between the beginning of experiment and roughly when the algorithm changed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of posts: {0}\".format(len(all_posts)))\n",
    "earliest_date\n",
    "time_before_algorithm_change = (parser.parse(\"2016-12-07 00:00:00\") - earliest_date)\n",
    "minutes_before_algorithm_change = time_before_algorithm_change.seconds/60 + time_before_algorithm_change.days * 1440\n",
    "print(\"{0} minutes elapsed between the beginning of experiment and roughly when the algorithm changed.\".format(minutes_before_algorithm_change))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Comments from Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#int(827/12)*12- 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...."
     ]
    }
   ],
   "source": [
    "dbcomments = []\n",
    "all_comments = []\n",
    "count = 0\n",
    "\n",
    "domains = set()\n",
    "\n",
    "querytext = \"\"\"\n",
    "SELECT * FROM archived_comments \n",
    "    JOIN experiment_things \n",
    "        ON experiment_things.id = archived_comments.post_id \n",
    "    WHERE experiment_id=8\n",
    "    AND experiment_things.created_at <= DATE_SUB( NOW() , INTERVAL 1 DAY )\n",
    "    AND experiment_things.created_at <= '2017-01-20 13:18:22'\n",
    "    AND object_type=1;\n",
    "\"\"\"\n",
    "\n",
    "for row in db_engine.execute(text(querytext)):\n",
    "# for row in db_engine.execute(text(\"select * from comments WHERE post_id IN (select posts.id from experiment_things JOIN posts on experiment_things.id = posts.id WHERE experiment_id=8 AND object_type=1);\")):\n",
    "    comment = {}\n",
    "    for key in row.keys():\n",
    "        comment[key] = row[key]\n",
    "    comment_data = json.loads(comment['comment_data'])\n",
    "    for key in comment_data.keys():\n",
    "        comment[key] = comment_data[key]\n",
    "    del comment['comment_data']\n",
    "    \n",
    "    comment['created'] = utc.localize(datetime.datetime.utcfromtimestamp(comment['created_utc']))\n",
    "    comment['body.length'] = len(comment['body'])\n",
    "    comment['link_count'] = 0  #comment['body'].count(\"http\")\n",
    "        \n",
    "#    comment['body'] = None\n",
    "    comment['body_html'] = None\n",
    "    comment['visible'] = True\n",
    "    if(comment['body'] == \"[removed]\"):\n",
    "        comment['visible'] = False\n",
    "    \n",
    "    all_comments.append(comment)\n",
    "    count += 1\n",
    "    if(count % 5000 == 0):\n",
    "        sys.stdout.write(\".\")\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(all_comments))\n",
    "#len([x['metadata_json'] for x in all_comments])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Recent Moderation Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recent_mod_actions = []\n",
    "for row in db_engine.execute(text('select action_data from mod_actions WHERE subreddit_id=\"2qh13\" AND created_utc >= \"02-01-2017\" ORDER BY created_utc;'.format(earliest_date))):\n",
    "    mod_action = json.loads(row['action_data'])\n",
    "    mod_action['created'] = utc.localize(datetime.datetime.utcfromtimestamp(mod_action['created_utc']))\n",
    "    recent_mod_actions.append(mod_action)\n",
    "print(\"{0} moderator actions loaded\".format(len(recent_mod_actions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Moderation Actions to Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all_comments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "mod_comment_actions = defaultdict(list)\n",
    "approved_count = 0\n",
    "removed_count = 0\n",
    "total_coments_removed_at_least_once = []\n",
    "comments_with_mod_actions = set()\n",
    "\n",
    "\n",
    "domain_mentions_unique_comments = {0:defaultdict(int),\n",
    "                                   1:defaultdict(int),\n",
    "                                   2:defaultdict(int)}\n",
    "excluded_domain_mentions_unique_comments = {0:defaultdict(int),\n",
    "                                   1:defaultdict(int),\n",
    "                                   2:defaultdict(int)}\n",
    "\n",
    "for action in recent_mod_actions:\n",
    "     if action['action'] == \"removecomment\" or action['action'] == \"approvecomment\":\n",
    "            comment_id = action['target_fullname'].replace(\"t1_\", \"\")\n",
    "            mod_comment_actions[comment_id].append(action)\n",
    "            comments_with_mod_actions.add(action['target_fullname'])\n",
    "\n",
    "print(\"{0} Total moderation actions\".format(sum([len(x) for x in mod_comment_actions.values()])))\n",
    "print(\"{0} Comments with moderation actions\".format(len(mod_comment_actions)))\n",
    "print(\"{0} Comments with more than one mod action\".format(len([x for x in mod_comment_actions.values() if len(x)>1])))\n",
    "print(\"\")\n",
    "\n",
    "for comment in all_comments:\n",
    "    if('later_deleted' not in comment.keys()):\n",
    "        comment['later_deleted'] = False\n",
    "        if(comment['author'] ==\"[deleted]\"):\n",
    "            comment['later_deleted'] = True\n",
    "    if comment['id'] in mod_comment_actions.keys():\n",
    "        for action in mod_comment_actions[comment['id']]:\n",
    "            ## many authors are later deleted, so try to \n",
    "            ## add in the author information here, since\n",
    "            ## the moderation log retains the author information\n",
    "            comment['author']  = action['target_author']\n",
    "            if action['action'] ==\"removecomment\":\n",
    "                removed_count += 1\n",
    "                total_coments_removed_at_least_once.append(comment['id'])\n",
    "                comment['visible'] = False\n",
    "            elif action['action'] == \"approvecomment\":\n",
    "                approved_count += 1\n",
    "                comment['visible']  = True\n",
    "                \n",
    "    metadata_json = json.loads(comment['metadata_json'])        \n",
    "    treatment = metadata_json['randomization']['treatment']\n",
    "    \n",
    "    ## increment the link count if it's not a link internally to reddit\n",
    "    for url in re.findall(r'(https?://\\S+)', comment['body']):\n",
    "        domain = urllib.parse.urlparse(url).netloc\n",
    "        domains.add(domain)\n",
    "\n",
    "        omit = False\n",
    "        for m in omitted_matches:\n",
    "            if(domain.find(m)>-1):\n",
    "                omit = True\n",
    "        if(omit!=True):\n",
    "            if(comment['visible']):\n",
    "                comment['link_count'] += 1\n",
    "                #total_links+= 1\n",
    "                domain_mentions_unique_comments[int(treatment)][domain] += 1\n",
    "            else:\n",
    "                excluded_domain_mentions_unique_comments[int(treatment)][domain] += 1 \n",
    "        else:\n",
    "            excluded_domain_mentions_unique_comments[int(treatment)][domain] += 1\n",
    "    \n",
    "\n",
    "\n",
    "                \n",
    "    del comment['metadata_json']\n",
    "    del comment['body']\n",
    "    \n",
    "print(\"Summary of Comment Visibility:\")\n",
    "print(Counter([x['visible'] for x in all_comments]))\n",
    "print(\"Took {0} actions to set a comment to removed\".format(removed_count))\n",
    "print(\"Took {0} actions to set a comment to approved\".format(approved_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Moderation Actions to Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_mod_actions = []\n",
    "matched_mod_actions = 0\n",
    "for action in recent_mod_actions:\n",
    "    if action['action'] == \"removelink\":\n",
    "        key = action['target_fullname'].replace(\"t3_\",\"\")\n",
    "        if key in all_posts.keys():\n",
    "            all_posts[key]['visible'] = False\n",
    "            matched_mod_actions += 1\n",
    "        else:\n",
    "            missing_mod_actions.append(key)\n",
    "    elif action['action'] == 'approvelink':\n",
    "        key = action['target_fullname'].replace(\"t3_\",\"\")\n",
    "        if key in all_posts.keys():\n",
    "            all_posts[key]['visible'] = True\n",
    "            matched_mod_actions += 1\n",
    "        else:\n",
    "            missing_mod_actions.append(key)\n",
    "#print(\"Missing Mod Actions: {0}\".format(len(missing_mod_actions)))\n",
    "#print(\"Missing Mod Action Posts: {0}\".format(len(set(missing_mod_actions))))\n",
    "print(\"Matched Mod Actions: {0}\".format(matched_mod_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Comment Count to Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post_comments = defaultdict(list)\n",
    "experiment_comments = []\n",
    "for comment in all_comments:\n",
    "    if(comment['author'] == \"CivilServantBot\"):\n",
    "        experiment_comments.append(comment)\n",
    "        continue\n",
    "    if(comment['author'] in bots):\n",
    "        continue\n",
    "    post_comments[comment['link_id']].append(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for post in all_posts.values():\n",
    "    metadata_json = json.loads(post['metadata_json'])        \n",
    "    post['post.treatment'] = metadata_json['randomization']['treatment']\n",
    "    post['post.assign.number']  = metadata_json['randomization'][\"\"]\n",
    "    post['post.block.id'] = metadata_json['randomization']['block.id']\n",
    "    post['post.block.size']  = metadata_json['randomization']['block.size']\n",
    "\n",
    "#    post['comment.links.total'] = 0\n",
    "#    post['comment.links.comments'] = 0\n",
    "    post['comments'] = len(post_comments[\"t3_\" + post['id']])\n",
    "#    if(\"t3_\" + post['id'] in post_comments.keys()):\n",
    "    post['comment.links.total'] = sum([x['link_count'] for x in post_comments[\"t3_\" + post['id']] if x['visible']])\n",
    "    post['comment.links.comments'] = len([x['link_count'] for x in post_comments[\"t3_\" + post['id']] if x['link_count']>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Post Information to Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#list(all_posts.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recent_comments_included = []\n",
    "experiment_comments = []\n",
    "for comment in all_comments:\n",
    "    if(comment['author'] == \"CivilServantBot\"):\n",
    "        experiment_comments.append(comment)\n",
    "        continue\n",
    "    if(comment['author'] in bots):\n",
    "        continue\n",
    "    post_id = comment['link_id'].replace(\"t3_\", \"\")\n",
    "    if(post_id in all_posts.keys()):\n",
    "        post = all_posts[comment['link_id'].replace(\"t3_\", \"\")]\n",
    "        post_created = utc.localize(post['created'])\n",
    "#        comment['post.ama'] = is_ama(post['link_flair_css_class'], post['block.id'])\n",
    "#        comment['post.badpost'] = is_badpost(post['link_flair_css_class'])\n",
    "#        comment['post.flair'] = strip_ama(post['link_flair_css_class'])\n",
    "        comment['post.created'] = post['created']\n",
    "        comment['minutes.since.post.created'] = (comment['created'] - post_created).total_seconds() / 60.\n",
    "#        comment['post.sub.top.minutes'] = post['post.sub.top.minutes']\n",
    "        comment['post.author'] = post['author']\n",
    "        comment['post.visible'] = post['visible']\n",
    "        comment['toplevel'] = comment['link_id'] == comment['parent_id']\n",
    "        comment['post.domain'] = post['domain']\n",
    "        comment['post.day.num']  = (post_created - utc.localize(datetime.datetime(1970,1,1))).days\n",
    "        comment['day.num'] = (comment['created'] - utc.localize(datetime.datetime(1970,1,1))).days\n",
    "        comment['weekday'] = comment['created'].weekday()\n",
    "        comment['weekend'] = (comment['weekday'] >=6)\n",
    "        \n",
    "        comment['post.comment.links.total']  = post['comment.links.total']\n",
    "        comment['post.comment.links.comments']  = post['comment.links.comments']\n",
    "        comment['post.comments'] = post['comments']\n",
    "        \n",
    "        metadata_json = json.loads(post['metadata_json'])        \n",
    "        comment['post.treatment'] = metadata_json['randomization']['treatment']\n",
    "        comment['post.assign.number']  = metadata_json['randomization'][\"\"]\n",
    "        comment['post.block.id'] = metadata_json['randomization']['block.id']\n",
    "        comment['post.block.size']  = metadata_json['randomization']['block.size']\n",
    "        recent_comments_included.append(comment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Comments Included: {0}\".format(len(recent_comments_included)))\n",
    "print(\"Experiment Comments: {0}\".format(len(experiment_comments)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "recent_comment_df = pd.DataFrame(recent_comments_included)\n",
    "recent_comments_filename = \"r_worldnews_comments_01.19.2017.a.csv\"\n",
    "recent_comment_df.to_csv(os.path.join(\"outputs\",recent_comments_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Data on The Score 24 Hours After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_text = \"\"\"\n",
    "select * from experiment_things JOIN\n",
    "  experiment_thing_snapshots ON\n",
    "  experiment_thing_snapshots.experiment_thing_id = experiment_things.id\n",
    "  WHERE experiment_things.experiment_id=8 AND experiment_things.object_type=1 AND\n",
    "  experiment_thing_snapshots.created_at <= DATE_ADD(experiment_things.created_at, INTERVAL 1 DAY ) AND\n",
    "  experiment_thing_snapshots.created_at >= DATE_ADD(experiment_things.created_at, INTERVAL 1420 MINUTE)\n",
    "  AND experiment_things.created_at <= DATE_SUB( NOW() , INTERVAL 1 DAY )\n",
    "  AND experiment_things.experiment_id=8\n",
    "  ORDER BY experiment_thing_snapshots.created_at ASC;\n",
    "\"\"\"\n",
    "\n",
    "score_snapshots = defaultdict(list)\n",
    "for row in db_engine.execute(text(query_text)):\n",
    "    metadata = json.loads(row['metadata_json'])\n",
    "    snapshot = {}\n",
    "    for key in row.keys():\n",
    "        snapshot[key] = row[key]\n",
    "    for key in metadata.keys():\n",
    "        snapshot[key] = metadata[key]\n",
    "    del snapshot['metadata_json']\n",
    "    score_snapshots[snapshot['experiment_thing_id']].append(snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "snapshotted_posts = []\n",
    "for p in all_posts.values():\n",
    "    post = copy.copy(p)\n",
    "    if post['id'] in list(score_snapshots.keys()):\n",
    "        snapshot = copy.copy(score_snapshots[post['id']][-1])\n",
    "        del snapshot['created_at']\n",
    "        del snapshot['object_created']\n",
    "        for key in snapshot.keys():\n",
    "            post['snapshot.' + key] = snapshot[key]\n",
    "        snapshotted_posts.append(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#len(snapshotted_posts)\n",
    "#snapshotted_posts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Scores From Dec 6 to Dec 7 To Test for Influence from Score Changes\n",
    "Decision: include a cutoff point for 21 hours into Dec 6 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_text = \"\"\"\n",
    "select * from experiment_things JOIN\n",
    "  experiment_thing_snapshots ON\n",
    "  experiment_thing_snapshots.experiment_thing_id = experiment_things.id\n",
    "  WHERE experiment_things.experiment_id=8 AND experiment_things.object_type=1 AND\n",
    "  experiment_thing_snapshots.created_at >= \"2016-12-06 00:00:00\"  AND\n",
    "  experiment_thing_snapshots.created_at <= \"2016-12-08 12:00:00\"\n",
    "  ORDER BY experiment_thing_snapshots.created_at ASC;\n",
    "\"\"\"\n",
    "#   AND experiment_thing_snapshots.created_at <= DATE_ADD(experiment_things.created_at, INTERVAL 1 DAY ) AND\n",
    "#   experiment_thing_snapshots.created_at >= DATE_ADD(experiment_things.created_at, INTERVAL 1420 MINUTE)\n",
    "\n",
    "d_scores = defaultdict(list)\n",
    "for row in db_engine.execute(text(query_text)):\n",
    "    metadata = json.loads(row['metadata_json'])\n",
    "    snapshot = {}\n",
    "    for key in row.keys():\n",
    "        snapshot[key] = row[key]\n",
    "    for key in metadata.keys():\n",
    "        snapshot[key] = metadata[key]\n",
    "    del snapshot['metadata_json']\n",
    "    d_scores[snapshot['experiment_thing_id']].append(snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#len(d_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Earliest Score For a Post After Algorithm Changes (Not Great, but a diagnostic of a kind), focusing only on scores available after noon on Dec 7 2016, well after the score transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for post in list(d_scores.values())[20:30]:\n",
    "    scores = []\n",
    "    minutes = []\n",
    "    for score in post:\n",
    "        diff = (score['created_at'] - parser.parse(\"2016-12-06 00:00:00\"))\n",
    "        scores.append(score['score'])\n",
    "        minute = diff.seconds/60 + diff.days*1440\n",
    "        minutes.append(minute)\n",
    "    plt.figure(figsize=(8,1))\n",
    "    plt.scatter(minutes, scores)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_text = \"\"\"\n",
    "select * from experiment_things JOIN\n",
    "  experiment_thing_snapshots ON\n",
    "  experiment_thing_snapshots.experiment_thing_id = experiment_things.id\n",
    "  WHERE experiment_thing_snapshots.created_at >= \"2016-12-07 12:00:00\"\n",
    "  AND experiment_things.created_at <= DATE_SUB( NOW() , INTERVAL 1 DAY )\n",
    "  AND experiment_things.experiment_id=8\n",
    "  ORDER BY experiment_thing_snapshots.created_at ASC;\n",
    "\"\"\"\n",
    "\n",
    "# since we are going in ascending order, we only need to store one at a time\n",
    "# newer scores will replace older ones\n",
    "youngest_score = {}\n",
    "for row in db_engine.execute(text(query_text)):\n",
    "    metadata = json.loads(row['metadata_json'])\n",
    "    snapshot = {}\n",
    "    for key in row.keys():\n",
    "        snapshot[key] = row[key]\n",
    "    for key in metadata.keys():\n",
    "        snapshot[key] = metadata[key]\n",
    "    del snapshot['metadata_json']\n",
    "    minutes_before_algorithm_change\n",
    "    if(snapshot['experiment_thing_id'] in youngest_score.keys() and \n",
    "       snapshot['experiment_thing_id'] in all_posts.keys()):\n",
    "        if((snapshot['created_at'] - \n",
    "            all_posts[snapshot['experiment_thing_id']]['created_at']) < \n",
    "           time_before_algorithm_change):\n",
    "            interval = (snapshot['created_at'] - all_posts[snapshot['experiment_thing_id']]['created_at'])\n",
    "            snapshot['later.score.interval.minutes'] = int(interval.days*1440 + interval.seconds/60)\n",
    "            youngest_score[snapshot['experiment_thing_id']] = snapshot\n",
    "    else:\n",
    "        try:\n",
    "            interval = (snapshot['created_at'] - all_posts[snapshot['experiment_thing_id']]['created_at'])\n",
    "            snapshot['later.score.interval.minutes'] = int(interval.days*1440 + interval.seconds/60)\n",
    "        except:\n",
    "            pass\n",
    "        youngest_score[snapshot['experiment_thing_id']] = snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for post in snapshotted_posts:\n",
    "    post['later.score'] = youngest_score[post['id']]['score']\n",
    "    post['later.score.interval.minutes'] = youngest_score[post['id']]['later.score.interval.minutes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Post Level Dataset For Analysis\n",
    "Includes only posts that had at least 24 hours since posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post_df = pd.DataFrame(snapshotted_posts)\n",
    "posts_filename = \"r_worldnews_posts_01.19.2017.a.csv\"\n",
    "post_df.to_csv(os.path.join(\"outputs\",posts_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#len(snapshotted_posts)\n",
    "#sorted(snapshotted_posts, key=lambda x: x['created'])[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Generate Max Rank For Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import csv\n",
    "# max_rows = {}\n",
    "# with open(\"outputs/worldnews_frontpages_max_2017-01-16.csv\", \"r\") as f:\n",
    "#     for row in csv.DictReader(f):\n",
    "#         if(row['sub_id']==' ' or row['sub_id'].strip()!=subreddit_id):\n",
    "#             continue\n",
    "#         max_rows[row['post_id']] = row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check to see if all experiment posts are in the max rows lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# included = 0\n",
    "# excluded = []\n",
    "# for post in snapshotted_posts:\n",
    "#     if post['id'] in max_rows.keys() and max_rows[post['id']]['SubredditPage HOT'] !=' ':\n",
    "#         included += 1\n",
    "#         post['max.hot'] = max_rows[post['id']]['SubredditPage HOT'].strip()\n",
    "#     else:\n",
    "#         post['max.hot'] = None\n",
    "#         excluded.append(post)\n",
    "# print(\"Included in top 200: {0}\".format(included))\n",
    "# print(\"Excluded from top 200: {0}\".format(len(excluded)))\n",
    "# print(\"Among Excluded, how many were removed:\")\n",
    "# print(\"     \" + str(Counter([x['visible'] for x in excluded])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# post_df = pd.DataFrame(snapshotted_posts)\n",
    "# posts_filename = \"r_worldnews_posts_01.17.2017.csv\"\n",
    "# post_df.to_csv(os.path.join(\"outputs\",posts_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Counter([x['max.hot'] for x in snapshotted_posts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Load Information about Score Over First 24 Hours\n",
    "Use only posts from the experiment that have been around for 24 hours or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_text = \"\"\"\n",
    "select * from experiment_things JOIN\n",
    "  experiment_thing_snapshots ON\n",
    "  experiment_thing_snapshots.experiment_thing_id = experiment_things.id\n",
    "  WHERE experiment_things.experiment_id=8 AND experiment_things.object_type=1 AND\n",
    "  DATE_ADD(experiment_things.created_at, INTERVAL 1 DAY) <= NOW() AND \n",
    "  experiment_thing_snapshots.created_at <= DATE_ADD(experiment_things.created_at, INTERVAL 1 DAY )\n",
    "  ORDER BY experiment_thing_snapshots.created_at ASC;\n",
    "\"\"\"\n",
    "\n",
    "tf_score_snapshots = defaultdict(list)\n",
    "for row in db_engine.execute(text(query_text)):\n",
    "    metadata = json.loads(row['metadata_json'])\n",
    "    snapshot = {}\n",
    "    for key in row.keys():\n",
    "        snapshot[key] = row[key]\n",
    "    for key in metadata.keys():\n",
    "        snapshot[key] = metadata[key]\n",
    "    del snapshot['metadata_json']\n",
    "    tf_score_snapshots[snapshot['experiment_thing_id']].append(snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#len(tf_score_snapshots)\n",
    "#list(tf_score_snapshots.values())[0]\n",
    "post_scores = []\n",
    "errors  =0 \n",
    "for scores in list(tf_score_snapshots.values()):\n",
    "    try:\n",
    "        post = all_posts[scores[0][\"experiment_thing_id\"]]\n",
    "        for score in scores:\n",
    "            p = copy.copy(post)\n",
    "            for key in score.keys():\n",
    "                p['score.'+key.replace(\"_\", \".\")] = score[key]\n",
    "            post_scores.append(p)\n",
    "    except:\n",
    "        errors +=1\n",
    "        pass\n",
    "print(\"{0} errors\".format(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(post_scores)\n",
    "scores_filename = \"r_worldnews_scores_01.20.2017.a.csv\"\n",
    "scores_df.to_csv(\"outputs/\" + scores_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Calculate Ranking Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step One: Creation Date For  All Posts in DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post_creation = {}\n",
    "for row in db_engine.execute(text(\"select * from posts WHERE subreddit_id = '2qh13';\")):\n",
    "    post_creation[row['id']] = row['created']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Two: Collect a dataset of all rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def timediff_minutes(timediff):\n",
    "    return timediff.seconds/60 + timediff.days*1440\n",
    "post_rankings = defaultdict(list)\n",
    "for page in db_engine.execute(text(\"select * from subreddit_pages WHERE subreddit_id='2qh13' AND page_type={0} AND created_at > '{1}' AND created_at < '{2}';\".format(\n",
    "    4, #PageType.HOT\n",
    "    earliest_date,\n",
    "    ranking_latest_date\n",
    "))):\n",
    "    posts = json.loads(page['page_data'])\n",
    "    median_score = np.median([x['score'] for x in posts])\n",
    "    post_ages = []\n",
    "    \n",
    "    \n",
    "    for post in posts:\n",
    "        try:\n",
    "            age = page.created_at - post_creation[post['id']]\n",
    "        except:\n",
    "            \n",
    "            submission = r.get_submission(submission_id = post['id'])\n",
    "            if(submission):\n",
    "                age = page.created_at - datetime.datetime.utcfromtimestamp(submission.created_utc)\n",
    "            else:\n",
    "                age = None\n",
    "        post_ages.append(timediff_minutes(age))\n",
    "    \n",
    "    ### FOR NOW OMIT MEDIAN AGE IF THERE'S MISSING INFO\n",
    "    try:\n",
    "        median_age = np.median(post_ages)\n",
    "    except:\n",
    "        median_age = None\n",
    "    position = 0\n",
    "\n",
    "    ### limit to the first 100 items\n",
    "    ### since we don't have 300 \n",
    "    ### until much later\n",
    "    for post in posts[0:100]:\n",
    "        try:\n",
    "            age = timediff_minutes(page.created_at - post_creation[post['id']])\n",
    "        except:\n",
    "            age = None\n",
    "        snapshot = {\"created_at\":page.created_at,\n",
    "                    \"snapshot.length\":len(posts),\n",
    "                    \"position\":position,\n",
    "                    \"median.score\":median_score,\n",
    "                    \"median.age\":median_age,\n",
    "                    \"post.score\":post['score'],\n",
    "                    \"post.age\": age,\n",
    "                    \"snapshot.comments\":post['num_comments'],\n",
    "                    \"post.id\":post['id']}\n",
    "        post_rankings[post['id']].append(snapshot)\n",
    "        position += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "removed_post_snapshots = 0\n",
    "for post in post_rankings.values():\n",
    "    for snapshot in post:\n",
    "        ## remove any entries that are older than 24 hours\n",
    "        if(snapshot['post.age']>60*24):\n",
    "            post.remove(snapshot)\n",
    "            removed_post_snapshots += 1\n",
    "print(\"Removed {0} post ranking snapshots older than 24 hours\".format(removed_post_snapshots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ranking_snapshot_count = sum([len(x) for x in list(post_rankings.values())])\n",
    "unique_post_count = len([x for x in list(post_rankings.values()) if len(x)>0])\n",
    "print(\"Current status: {0} snapshots of {1} posts\".format(ranking_snapshot_count, unique_post_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create A Max Rank Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for post in snapshotted_posts:\n",
    "    if post['id'] in post_rankings.keys():\n",
    "        rankings = post_rankings[post['id']]\n",
    "        topmost_rank = rankings[0]['position']\n",
    "        for ranking in rankings:\n",
    "            if(topmost_rank > ranking['position']):\n",
    "                topmost_rank = ranking['position']\n",
    "        max_rank = abs(101 - topmost_rank)\n",
    "    else:\n",
    "        max_rank = 0\n",
    "    post['max.rank'] = max_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#snapshotted_posts[0]\n",
    "print(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist([x['max.rank'] for x in snapshotted_posts if x['post.treatment']=='0'])\n",
    "plt.show()\n",
    "plt.hist([x['max.rank'] for x in snapshotted_posts if x['post.treatment']=='1'])\n",
    "plt.show()\n",
    "plt.hist([x['max.rank'] for x in snapshotted_posts if x['post.treatment']=='2'])\n",
    "plt.show()\n",
    "# #np.vstack([x, y]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### NOW WRITE RANKED POSTS TO FILE\n",
    "ranked_post_df = pd.DataFrame(snapshotted_posts)\n",
    "ranked_posts_filename = \"r_worldnews_posts_ranked_01.19.2017.a.csv\"\n",
    "ranked_post_df.to_csv(os.path.join(\"outputs\",ranked_posts_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Ranking Dataset of Experiment Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment_post_rankings = []\n",
    "for post in snapshotted_posts:\n",
    "    ranking_info = copy.copy(post_rankings[post['id']])\n",
    "    for snapshot in ranking_info:\n",
    "        for key in post.keys():\n",
    "            if(key not in [\"metadata_json\", \"report_reasons\", \"mod_reports\", \"secure_media_embed\"]):\n",
    "                snapshot['post.'+key] = post[key]\n",
    "        experiment_post_rankings.append(snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#experiment_post_rankings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_rankings_df = pd.DataFrame(experiment_post_rankings)\n",
    "exp_rankings_filename = \"r_worldnews_exp_rankings_01.19.2017.a.csv\"\n",
    "exp_rankings_df.to_csv(\"outputs/\" + exp_rankings_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Ranking Dataset of All Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_rankings = []\n",
    "for rankings in post_rankings.values():\n",
    "    all_rankings.extend(rankings)\n",
    "rankings_df = pd.DataFrame(all_rankings)\n",
    "rankings_filename = \"r_worldnews_rankings_01.19.2017.csv\"\n",
    "rankings_df.to_csv(\"outputs/\" + rankings_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_rankings[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Understanding the Context\n",
    "* Analyze the comments\n",
    "* Analyze the posts themselves (confirmed that the rejection or submission rates haven't changed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Analyze Domain Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in [0,1,2]: \n",
    "    tabloid = 0\n",
    "    msm = 0\n",
    "    other = 0\n",
    "    for domain in domain_mentions_unique_comments[i]:\n",
    "        matched = False\n",
    "        for td in tabloid_domains:\n",
    "            if(domain.find(td)!=-1):\n",
    "                tabloid += 1\n",
    "                matched = True\n",
    "        for md in msm_domains:\n",
    "            if(domain.find(md)!=-1):\n",
    "                msm += 1\n",
    "                matched = True\n",
    "        if(matched!=True):\n",
    "            other += 1\n",
    "                \n",
    "    print(\"Experiment Arm {0}\".format(i))\n",
    "    print(\"{0} Tabloid links\".format(tabloid))\n",
    "    print(\"{0} MSM links\".format(msm))\n",
    "    print(\"{0} other links\".format(other))\n",
    "    #sorted(domain_mentions_unique_comments[0].items(),key=lambda x: x[1], reverse=True)[0:25]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def threelist():\n",
    "    return {0:0,1:0,2:0, \"domain\":\"\"}\n",
    "domain_arms = defaultdict(threelist)\n",
    "for i in [0,1,2]:\n",
    "    for item in domain_mentions_unique_comments[i].items():\n",
    "        domain = item[0]\n",
    "        omit = False\n",
    "        for m in omitted_matches:\n",
    "            if(domain.find(m)>-1):\n",
    "                omit = True\n",
    "        if(omit!=True):\n",
    "            domain_arms[item[0]][i] = item[1]\n",
    "            domain_arms[item[0]][\"domain\"] = domain\n",
    "domain_arms_df = pd.DataFrame(list(domain_arms.values()))\n",
    "domain_arms_filename = \"r_worldnews_domain_arms_01.19.2017.csv\"\n",
    "domain_arms_df.to_csv(os.path.join(\"outputs\",domain_arms_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def threelist():\n",
    "    return {0:0,1:0,2:0, \"domain\":\"\"}\n",
    "excluded_domain_arms = defaultdict(threelist)\n",
    "for i in [0,1,2]:\n",
    "    for item in excluded_domain_mentions_unique_comments[i].items():\n",
    "        domain = item[0]\n",
    "        excluded_domain_arms[item[0]][i] = item[1]\n",
    "        excluded_domain_arms[item[0]][\"domain\"] = domain\n",
    "excluded_domain_arms_df = pd.DataFrame(list(excluded_domain_arms.values()))\n",
    "excluded_domain_arms_filename = \"r_worldnews_domains_excluded_arms_01.19.2017.csv\"\n",
    "excluded_domain_arms_df.to_csv(os.path.join(\"outputs\",excluded_domain_arms_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#excluded_domain_arms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remaining Tasks:\n",
    "\n",
    "* Confirm which blocks to include and exclude (done)\n",
    "* Include and exclude those blocks (done)\n",
    "* Recalculate existing analyses (done)\n",
    "* Do the score predictions (in progress)\n",
    "* Make a decision about score and the algorithm change\n",
    "* Include info on excluded domains\n",
    "* Include a random sample of headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Counter([x['post.block.id'] for x in snapshotted_posts if x['later.score.interval.minutes']>=13000])\n",
    "Counter([x['post.block.id'] for x in snapshotted_posts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(23458233)\n",
    "\n",
    "removed = random.sample([x for x in snapshotted_posts if x['visible'] is False], 40)\n",
    "kept = random.sample([x for x in snapshotted_posts if x['visible'] is True], 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "removed_submissions = []\n",
    "for submission in r.get_submissions([x['name'] for x in removed]):\n",
    "    removed_submissions.append(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in removed_submissions[0:50]:\n",
    "    print(\"* [*{0}*] {1}\".format(i.json_dict['link_flair_text'], i.json_dict['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in kept[0:10]:\n",
    "    print(\"* {0}\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who Did the Fact-Checking?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author_links = 0\n",
    "commenters = []\n",
    "for comment in recent_comments_included:\n",
    "    if(comment['link_count']>0):\n",
    "        commenters.append(comment['author'])\n",
    "        if(comment['author']==comment['post.author']):\n",
    "            author_links += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"{0} comments with links\".format(len(commenters)))\n",
    "print(\"{0} unique commenters\".format(len(set(commenters))))\n",
    "print(\"{0} of these comments were made by the post author\".format(author_links))\n",
    "#plt.hist([x[1] for x in Counter(commenters).items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comment_list = sorted(Counter(commenters).items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len([x for x in comment_list if x[1]>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(domain_mentions_unique_comments[1].items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
