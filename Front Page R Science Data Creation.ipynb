{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import inspect, os, sys, copy, pytz, re, glob\n",
    "import simplejson as json\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt   # Matplotlib for plotting\n",
    "import matplotlib.dates as md\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pybloom\n",
    "from collections import Counter, defaultdict\n",
    "utc=pytz.UTC\n",
    "\n",
    "ENV = \"production\"\n",
    "os.environ['CS_ENV'] = 'production'\n",
    "BASE_DIR = \"/home/nathan/CivilServant\"\n",
    "sys.path.append(BASE_DIR)\n",
    "\n",
    "with open(os.path.join(BASE_DIR, \"config\") + \"/{env}.json\".format(env=ENV), \"r\") as config:\n",
    "  DBCONFIG = json.loads(config.read())\n",
    "\n",
    "### LOAD SQLALCHEMY\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import text\n",
    "\n",
    "db_engine = create_engine(\"mysql://{user}:{password}@{host}/{database}\".format(\n",
    "    host = DBCONFIG['host'],\n",
    "    user = DBCONFIG['user'],\n",
    "    password = DBCONFIG['password'],\n",
    "    database = DBCONFIG['database']))\n",
    "DBSession = sessionmaker(bind=db_engine)\n",
    "\n",
    "### LOAD PRAW\n",
    "import reddit.connection\n",
    "conn = reddit.connection.Connect(base_dir=BASE_DIR, env=\"jupyter\")\n",
    "\n",
    "### FILTER OUT DEPRECATION WARNINGS ASSOCIATED WITH DECORATORS\n",
    "# https://github.com/ipython/ipython/issues/9242\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning, message='.*use @default decorator instead.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals of this Notebook\n",
    "The goal of this notebook is to create a dataset that can be used to test hypotheses about the importance of reddit top page placement time on the number of newcomer comments and newcomer comment removals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PRAW session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "substituting https://oauth.reddit.com for https://api.reddit.com in url\n",
      "GET: https://oauth.reddit.com/api/v1/me.json\n"
     ]
    }
   ],
   "source": [
    "r = conn.connect(controller=\"ModLog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Dates to Consider for this Analysis\n",
    "To set this up, I ran the following query:\n",
    "\n",
    "<em>select count(*), DATE(created_at) FROM subreddit_pages WHERE page_type=1 GROUP BY DATE(created_at);</em>\n",
    "\n",
    "<pre>\n",
    "+----------+------------------+\n",
    "| count(*) | DATE(created_at) |\n",
    "+----------+------------------+\n",
    "|       15 | 2016-06-17       |\n",
    "|      383 | 2016-06-28       |\n",
    "|      719 | 2016-06-29       |\n",
    "|      718 | 2016-06-30       |\n",
    "|      663 | 2016-07-01       |\n",
    "<span style=\"font-weight:bold;\">|      178 | 2016-07-04       |</span>\n",
    "|      720 | 2016-07-05       |\n",
    "|      548 | 2016-07-06       |\n",
    "|      358 | 2016-07-07       |\n",
    "|      358 | 2016-07-08       |\n",
    "|      359 | 2016-07-09       |\n",
    "|      360 | 2016-07-10       |\n",
    "|      360 | 2016-07-11       |\n",
    "|      358 | 2016-07-12       |\n",
    "|      360 | 2016-07-13       |\n",
    "|      359 | 2016-07-14       |\n",
    "|      359 | 2016-07-15       |\n",
    "|      360 | 2016-07-16       |\n",
    "|      360 | 2016-07-17       |\n",
    "|      358 | 2016-07-18       |\n",
    "|      354 | 2016-07-19       |\n",
    "|      358 | 2016-07-20       |\n",
    "|      360 | 2016-07-21       |\n",
    "|      360 | 2016-07-22       |\n",
    "|      359 | 2016-07-23       |\n",
    "|      360 | 2016-07-24       |\n",
    "|      358 | 2016-07-25       |\n",
    "|      359 | 2016-07-26       |\n",
    "|      359 | 2016-07-27       |\n",
    "|      360 | 2016-07-28       |\n",
    "|      359 | 2016-07-29       |\n",
    "|      360 | 2016-07-30       |\n",
    "|      360 | 2016-07-31       |\n",
    "|      280 | 2016-08-01       |\n",
    "+----------+------------------+\n",
    "34 rows in set (0.75 sec)\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "earlybound = \"2016-07-01\"\n",
    "dates = ['2016-07-05','2016-07-06','2016-07-07',\n",
    "         '2016-07-08','2016-07-09','2016-07-10',\n",
    "         '2016-07-11','2016-07-12','2016-07-13',\n",
    "         '2016-07-14','2016-07-15','2016-07-16',\n",
    "         '2016-07-17','2016-07-18','2016-07-19',\n",
    "         '2016-07-20','2016-07-21','2016-07-22',\n",
    "         '2016-07-23','2016-07-24','2016-07-25',\n",
    "         '2016-07-26','2016-07-27','2016-07-28',\n",
    "         '2016-07-29','2016-07-30','2016-07-31'\n",
    "        ]\n",
    "included_dates = [parser.parse(x).date() for x in dates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset of Posts Appearing on the Front Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subreddit_pages = []\n",
    "for row in db_engine.execute(text('select * from subreddit_pages WHERE page_type=1 AND created_at>=\"2016-07-05\" AND created_at <=\"2017-07-31\" ORDER BY created_at;')):\n",
    "    subreddit_page = {}\n",
    "    subreddit_page['page']= json.loads(row['page_data'])\n",
    "    for key in row.keys():\n",
    "        subreddit_page[key] = row[key]\n",
    "    if(subreddit_page['subreddit_id']==\"mouw\" and subreddit_page['created_at'].date() in included_dates):\n",
    "        subreddit_pages.append(subreddit_page)\n",
    "print(\"Found {0} subreddit pages\".format(len(subreddit_pages)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_top_posts(page, n=10):\n",
    "    return([x['id'] for x in page[0:n]])\n",
    "\n",
    "def get_top_post_dates(page, n=10):\n",
    "    dates = sorted([utc.localize(datetime.datetime.utcfromtimestamp(x['created_utc'])) for x in page[0:n]])\n",
    "    return({\"first\":dates[0], \"last\":dates[-1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timediffs = []\n",
    "last_time = None\n",
    "for page in subreddit_pages:\n",
    "    current_time = page['created_at']\n",
    "    if(last_time is not None):\n",
    "        timediffs.append((current_time - last_time).total_seconds())\n",
    "    last_time = current_time\n",
    "\n",
    "plt.figure(figsize=(10, 3)) \n",
    "plt.hist(timediffs, bins=50)\n",
    "plt.title(\"Histogram of elapsed time between samples of r/science/top\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post_minutes_on_top = defaultdict(int)\n",
    "last_time = None\n",
    "top_n = 10\n",
    "for page in subreddit_pages:\n",
    "    current_time = page['created_at']\n",
    "    if(last_time is not None):\n",
    "        estimated_time = (current_time - last_time).total_seconds() / 60.\n",
    "    else:\n",
    "        estimated_time = np.mean(timediffs)  / 60.\n",
    "    \n",
    "    for post in get_top_posts(page['page'], n=top_n):\n",
    "        post_minutes_on_top[post] += estimated_time\n",
    "\n",
    "    \n",
    "    last_time = current_time\n",
    "\n",
    "print(\"{0} Total pages analyzed\".format(len(subreddit_pages)))\n",
    "print(\"{0} Total posts appeared on the Top {1} listing for r/science\".format(len(post_minutes_on_top), top_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3)) \n",
    "plt.hist([x for x in post_minutes_on_top.values()], bins=50)\n",
    "plt.title(\"Minutes In the top {0} listing for r/science\".format(top_n), fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Baumgartner Posts from the specified dates (includes some gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post_ids = pybloom.BloomFilter(capacity=10000000, error_rate = .000000001)\n",
    "science_posts = {}\n",
    "with open(\"/mnt/corsair/reddit_archive/baumgartner-bigquery-data/baumgartner_science_posts_2016_08_01_1900.json\", \"r\") as lines:\n",
    "    for line in lines:\n",
    "        post = json.loads(line)\n",
    "        post['created'] = parser.parse(post['created_utc'])\n",
    "        if(post['created'] >= utc.localize(parser.parse(earlybound)) and post['created'] <= utc.localize(parser.parse(dates[-1]))):\n",
    "            post['day.num'] = (post['created'] - utc.localize(datetime.datetime(1970,1,1))).days\n",
    "            full_id = \"t3_\" + post['id']\n",
    "            if(full_id in post_minutes_on_top):\n",
    "                post['post.sub.top.minutes'] = post_minutes_on_top[full_id]\n",
    "            else:\n",
    "                post['post.sub.top.minutes'] = 0\n",
    "            science_posts[post['id']] = post\n",
    "            post_ids.add(post['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out Posts From Before the Sample Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dates = [] \n",
    "for post in subreddit_pages[0]['page']:\n",
    "    dates.append(science_posts[post['id']]['created'])\n",
    "\n",
    "earliest_date = sorted(dates)[0]\n",
    "latest_date = None\n",
    "removed_posts = 0\n",
    "for post_id in list(science_posts.keys()):\n",
    "    if science_posts[post_id]['created'] < earliest_date:\n",
    "        del science_posts[post_id]\n",
    "        removed_posts += 1\n",
    "    if(post_id in science_posts.keys() and (latest_date is None or science_posts[post_id]['created']>latest_date)):\n",
    "        latest_date = science_posts[post_id]['created']\n",
    "        \n",
    "print(\"Removed {0} posts for being too early\".format(removed_posts))\n",
    "print(\"{0} total science posts remaining\".format(len(science_posts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Pandas Dataframe from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posts_df = pd.DataFrame(list(science_posts.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot posts per day in the the sample period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = pd.DataFrame(posts_df.created.values, index=posts_df.created.values.astype('datetime64'))\n",
    "fig, ax = plt.subplots(figsize=(20,5))\n",
    "x.resample(\"D\").count().plot(rot=45, ax=ax)\n",
    "plt.ylim([0,x.resample(\"D\").count()[0].max() + 10])\n",
    "plt.title(\"Total number of post submissions per day, r/science, in reddit + baumgartner data (n={0})\".format(posts_df.created.count()), fontsize=\"24\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Moderator Actions Going Back to The Beginning of the Sample Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recent_mod_actions = []\n",
    "for row in db_engine.execute(text('select action_data from mod_actions WHERE subreddit_id=\"mouw\" AND created_utc >= \"{0}\" ORDER BY created_utc;'.format(earliest_date))):\n",
    "    mod_action = json.loads(row['action_data'])\n",
    "    mod_action['created'] = utc.localize(datetime.datetime.utcfromtimestamp(mod_action['created_utc']))\n",
    "    recent_mod_actions.append(mod_action)\n",
    "print(\"{0} moderator actions loaded\".format(len(recent_mod_actions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag posts as visible or non-visible based on moderation log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posts_2016 = {}\n",
    "for post in posts_df[posts_df.created>=earliest_date].to_dict(\"records\"):\n",
    "    post['visible'] = True\n",
    "    posts_2016[post['id']] = post\n",
    "recent_post_count = len(posts_2016.values())\n",
    "print(\"Recent Post Count: {0}\".format(recent_post_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find missing posts mentioned in the moderation log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_mod_actions = []\n",
    "matched_mod_actions = 0\n",
    "for action in recent_mod_actions:\n",
    "    if action['action'] == \"removelink\":\n",
    "        key = action['target_fullname'].replace(\"t3_\",\"\")\n",
    "        if key in posts_2016.keys():\n",
    "            posts_2016[key]['visible'] = False\n",
    "            matched_mod_actions += 1\n",
    "        else:\n",
    "            missing_mod_actions.append(key)\n",
    "    elif action['action'] == 'approvelink':\n",
    "        key = action['target_fullname'].replace(\"t3_\",\"\")\n",
    "        if key in posts_2016.keys():\n",
    "            posts_2016[key]['visible'] = True\n",
    "            matched_mod_actions += 1\n",
    "        else:\n",
    "            missing_mod_actions.append(key)\n",
    "print(\"Missing Mod Actions: {0}\".format(len(missing_mod_actions)))\n",
    "print(\"Missing Mod Action Posts: {0}\".format(len(set(missing_mod_actions))))\n",
    "print(\"Matched Mod Actions: {0}\".format(matched_mod_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch missing posts from reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extra_posts = []\n",
    "for submission in r.get_submissions([\"t3_\" + x for x in set(missing_mod_actions)]):\n",
    "    extra_posts.append(submission.json_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### AND NO NEED TO GO BACK THROUGH THE MODERATION ACTION LIST\n",
    "### FOR THESE, SINCE THEY WILL HAVE THE banned_by PROPERTY\n",
    "\n",
    "outside_time_window = 0\n",
    "within_time_window = 0\n",
    "tail = latest_date\n",
    "\n",
    "for post in extra_posts:\n",
    "    post['created'] = utc.localize(datetime.datetime.utcfromtimestamp(int(post['created_utc'])))\n",
    "    if post['created'] >= recent_mod_actions[0]['created'] and post['created'] <= tail:\n",
    "        if(post['banned_by'] is not None):\n",
    "            post['visible'] = False\n",
    "        else:\n",
    "            post['visible'] = True\n",
    "        post['post.sub.top.minutes'] = 0\n",
    "        posts_2016[post['id']] = post\n",
    "        within_time_window += 1\n",
    "    else:\n",
    "        outside_time_window += 1\n",
    "    \n",
    "print(\"Extra posts within time window: {0}\".format(within_time_window))\n",
    "print(\"Extra posts outside time window: {0}\".format(outside_time_window))\n",
    "    \n",
    "for post in posts_2016.values():\n",
    "    post['day.num'] = (post['created'] - utc.localize(datetime.datetime(1970,1,1))).days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Create Dataframe to Chart Submitted versus Permitted Posts Per Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recent_posts = pd.DataFrame(list(posts_2016.values()))\n",
    "print(\"Posts before update from modlog: {0}\".format(recent_post_count))\n",
    "print(\"Posts after update from modlog: {0}\".format(recent_posts.created.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Posts Per Day in Recent History of Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### TOTAL SUBMISSIONS\n",
    "df = recent_posts[((recent_posts.created <= latest_date) &\n",
    "                   (recent_posts.created >= earliest_date))]\n",
    "x = pd.DataFrame(df.created.values, index=df.created.values.astype('datetime64'))\n",
    "fig, ax = plt.subplots(figsize=(20,5))\n",
    "x.resample(\"D\").count().plot(rot=45, ax=ax)\n",
    "plt.ylim([0,x.resample(\"D\").count()[0].max() + 10])\n",
    "plt.title(\"Total number of post submissions per day, r/science, in reddit + baumgartner data (n={0})\".format(df.created.count()), fontsize=\"24\")\n",
    "plt.show()\n",
    "\n",
    "total_counts = x.resample(\"D\").count().to_dict(\"records\")\n",
    "\n",
    "### NOW ACCEPTED SUBMISSIONS\n",
    "df = recent_posts[((recent_posts.visible == True) & \n",
    "                   (recent_posts.created <= latest_date) &\n",
    "                   (recent_posts.created >= earliest_date))]\n",
    "x = pd.DataFrame(df.created.values, index=df.created.values.astype('datetime64'))\n",
    "fig, ax = plt.subplots(figsize=(20,5))\n",
    "x.resample(\"D\").count().plot(rot=45, ax=ax)\n",
    "plt.ylim([0,x.resample(\"D\").count()[0].max() + 10])\n",
    "plt.title(\"Total number of visible submissions per day, r/science, in reddit + baumgartner data (n={0})\".format(df.created.count()), fontsize=\"24\")\n",
    "plt.show()\n",
    "\n",
    "retained_counts = x.resample(\"D\").count().to_dict(\"records\")\n",
    "\n",
    "### NOW REMOVED SUBMISSIONS\n",
    "df = recent_posts[((recent_posts.visible == False) & \n",
    "                   (recent_posts.created <= latest_date) &\n",
    "                   (recent_posts.created >= earliest_date))]\n",
    "x = pd.DataFrame(df.created.values, index=df.created.values.astype('datetime64'))\n",
    "fig, ax = plt.subplots(figsize=(20,5))\n",
    "x.resample(\"D\").count().plot(rot=45, ax=ax)\n",
    "plt.ylim([0,x.resample(\"D\").count()[0].max() + 10])\n",
    "plt.title(\"Total number of removed submissions per day, r/science, in reddit + baumgartner data (n={0})\".format(df.created.count()), fontsize=\"24\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Dependent Variables: \n",
    "* newcomer comments on posts that are allowed to remain\n",
    "* the number of newcomer comments on posts that are allowed to remain\n",
    "* the number of removed newcomer comments on posts that are allowed to remain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load last six months of comments from official reddit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comment_head = parser.parse(\"2015-12-01 00:00:00 UTC\")\n",
    "all_comments = {}\n",
    "comment_ids = pybloom.BloomFilter(capacity=10000000, error_rate = .000000001)\n",
    "\n",
    "for filename in [#'/mnt/corsair/reddit_archive/official-bigquery-data/science_comments_12_2015.json',\n",
    "                 '/mnt/corsair/reddit_archive/official-bigquery-data/science_comments_01_2016.json',\n",
    "                 '/mnt/corsair/reddit_archive/official-bigquery-data/science_comments_02_2016.json',\n",
    "                 '/mnt/corsair/reddit_archive/official-bigquery-data/science_comments_03_2016.json',\n",
    "                 '/mnt/corsair/reddit_archive/official-bigquery-data/science_comments_04_2016.json',\n",
    "                 '/mnt/corsair/reddit_archive/official-bigquery-data/science_comments_05_2016.json']:\n",
    "    sys.stdout.write(\".\")\n",
    "    sys.stdout.flush()\n",
    "    with open(filename, \"r\") as lines:\n",
    "        for line in lines:\n",
    "            comment = json.loads(line)\n",
    "            if(comment['id'] not in comment_ids):\n",
    "                comment['created'] = utc.localize(datetime.datetime.utcfromtimestamp(float(comment['created_utc'])))\n",
    "                comment['visible'] = True\n",
    "                if(comment['body'] == \"[removed]\"):\n",
    "                    comment['visible'] = False\n",
    "                comment['body.length'] = len(comment['body'])\n",
    "                comment['body'] = None\n",
    "                comment['body_html'] = None\n",
    "                all_comments[comment['id']] = comment\n",
    "                comment_ids.add(comment['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Baumgartner Comments from the last month\n",
    "Unlike the post data, I checked this and found no gaps, at least going from the moderation log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Omitting tails for this analysis\n",
    "#tail = parser.parse('2016-05-30 23:59:59 UTC')\n",
    "count = 0\n",
    "with open(\"/mnt/corsair/reddit_archive/baumgartner-bigquery-data/baumgartner_science_comments_2016_08_01_1900.json\", \"r\") as comment_file:\n",
    "    for line in comment_file:\n",
    "        comment = json.loads(line)\n",
    "        if(comment['id'] not in comment_ids):\n",
    "            comment['created'] = parser.parse(comment['created_utc'])\n",
    "#            if(comment['created'] <= tail):\n",
    "            comment['body.length'] = len(comment['body'])\n",
    "            comment['body'] = None\n",
    "            comment['body_html'] = None\n",
    "            comment['visible'] = True\n",
    "            if(comment['body'] == \"[removed]\"):\n",
    "                comment['visible'] = False\n",
    "            all_comments[comment['id']] = comment\n",
    "            comment_ids.add(comment['id'])\n",
    "        count += 1\n",
    "        if(count % 50000 == 0):\n",
    "            sys.stdout.write(\".\")\n",
    "            sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Missing Coments from Moderation Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_comment_ids = []\n",
    "for action in recent_mod_actions:\n",
    "    if action['target_fullname'] is not None and \"t1_\" in action['target_fullname']:\n",
    "        if action['target_fullname'].replace(\"t1_\", \"\") not in all_comments.keys():\n",
    "            missing_comment_ids.append(action['target_fullname'])\n",
    "print(\"Missing Comments: {0}\".format(len(missing_comment_ids)))\n",
    "\n",
    "### FETCH COMMENT INFORMATION FOR ALL MISSING COMMENTS\n",
    "missing_comments = []\n",
    "counter = 0\n",
    "for comment_obj in r.get_info(thing_id=missing_comment_ids):\n",
    "    comment = comment_obj.json_dict\n",
    "    comment['created'] = utc.localize(datetime.datetime.utcfromtimestamp(float(comment['created_utc'])))\n",
    "    comment['visible'] = True\n",
    "    if(comment['body'] == \"[removed]\" or comment['banned_by'] is not None):\n",
    "        comment['visible'] = False\n",
    "    comment['body.length'] = len(comment['body'])\n",
    "    comment['body'] = None\n",
    "    comment['body_html'] = None\n",
    "    missing_comments.append(comment)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Found {0} additional comments moderated during the newcomer period and moderation log period\".format(len(missing_comments)))\n",
    "print(\"{0} of these comments have the author [deleted]\".format(len([x for x in missing_comments if x['author']==\"[deleted]\"])))\n",
    "print(\"{0} of these comments were removed.\".format(len([x for x in missing_comments if x['visible']==False])))\n",
    "mod_action_head = recent_mod_actions[0]['created']\n",
    "comments_in_comment_period = len([x for x in missing_comments if (mod_action_head < x['created'] and tail >= x['created'])])\n",
    "print(\"{0} of these fall within the period covered by the moderation log.\".format(comments_in_comment_period))\n",
    "\n",
    "newcomer_head = parser.parse(\"2016-01-01 00:00:00 UTC\")\n",
    "comments_in_newcomer_period = len([x for x in missing_comments if (newcomer_head < x['created'] and tail >= x['created'])])\n",
    "print(\"{0} of these fall within the period used for calculating newcomers.\".format(comments_in_newcomer_period))\n",
    "\n",
    "added_count = 0\n",
    "for comment in missing_comments:\n",
    "    if(comment['created'] >= newcomer_head and comment['created'] <= tail):\n",
    "        all_comments[comment['id']] = comment\n",
    "        added_count +=1\n",
    "print(\"\")\n",
    "print(\"{0} comments added to all_comments\".format(added_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a time sorted list of comments on the sampled posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "EPOCH = utc.localize(datetime.datetime.utcfromtimestamp(0))\n",
    "\n",
    "class CommentHeapObj(object):\n",
    "    def __init__(self, comment):\n",
    "        self.index = int((comment['created'] - EPOCH).total_seconds())\n",
    "        self.val = comment\n",
    "    def __lt__(self, other):\n",
    "        return self.index < other.index\n",
    "\n",
    "def heapsort(comments):\n",
    "    h = []\n",
    "    for comment in comments:\n",
    "        heapq.heappush(h, CommentHeapObj(comment))\n",
    "    return [heapq.heappop(h).val for i in range(len(h))]\n",
    "\n",
    "all_comments = heapsort(all_comments.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Moderation Actions to Comments, Setting Comments as Visible or Not Visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "mod_comment_actions = defaultdict(list)\n",
    "approved_count = 0\n",
    "removed_count = 0\n",
    "total_coments_removed_at_least_once = []\n",
    "comments_with_mod_actions = set()\n",
    "\n",
    "for action in recent_mod_actions:\n",
    "     if action['action'] == \"removecomment\" or action['action'] == \"approvecomment\":\n",
    "            comment_id = action['target_fullname'].replace(\"t1_\", \"\")\n",
    "            mod_comment_actions[comment_id].append(action)\n",
    "            comments_with_mod_actions.add(action['target_fullname'])\n",
    "\n",
    "print(\"{0} Total moderation actions\".format(sum([len(x) for x in mod_comment_actions.values()])))\n",
    "print(\"{0} Comments with moderation actions\".format(len(mod_comment_actions)))\n",
    "print(\"{0} Comments with more than one mod action\".format(len([x for x in mod_comment_actions.values() if len(x)>1])))\n",
    "print(\"\")\n",
    "\n",
    "for comment in all_comments:\n",
    "    if('later_deleted' not in comment.keys()):\n",
    "        comment['later_deleted'] = False\n",
    "        if(comment['author'] ==\"[deleted]\"):\n",
    "            comment['later_deleted'] = True\n",
    "    if comment['id'] in mod_comment_actions.keys():\n",
    "        for action in mod_comment_actions[comment['id']]:\n",
    "            ## many authors are later deleted, so try to \n",
    "            ## add in the author information here, since\n",
    "            ## the moderation log retains the author information\n",
    "            comment['author']  = action['target_author']\n",
    "            if action['action'] ==\"removecomment\":\n",
    "                removed_count += 1\n",
    "                total_coments_removed_at_least_once.append(comment['id'])\n",
    "                comment['visible'] = False\n",
    "            elif action['action'] == \"approvecomment\":\n",
    "                approved_count += 1\n",
    "                comment['visible']  = True\n",
    "print(\"Summary of Comment Visibility:\")\n",
    "print(Counter([x['visible'] for x in all_comments]))\n",
    "print(\"Took {0} actions to set a comment to removed\".format(removed_count))\n",
    "print(\"Took {0} actions to set a comment to approved\".format(approved_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Author Comment Number to All Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author_comment_num = defaultdict(int)\n",
    "\n",
    "for comment in all_comments:\n",
    "    comment['author.prev.comments'] = author_comment_num[comment['author']]    \n",
    "    author_comment_num[comment['author']] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Author Removed Count to All Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "author_removed_num = defaultdict(int)\n",
    "\n",
    "for comment in all_comments:\n",
    "    comment['author.prev.removed'] = author_removed_num[comment['author']]  \n",
    "    if(comment['visible']==False):\n",
    "        author_removed_num[comment['author']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "plt.figure(figsize=(10, 3)) \n",
    "plt.hist([math.log1p(x['author.prev.removed']) for x in all_comments])\n",
    "plt.title(\"log1p Number of author's previous comments removed, by comment\", fontsize=\"18\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataframes for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset of Comments from The Front Page Observation Period forward\n",
    "Tagged comments with information about the post they were attached to, including:\n",
    "* were they top-level comments or replies?\n",
    "* what flair did the post have\n",
    "* what time was the post made\n",
    "* was the post removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## TODO: set head and tail for comments dataset.\n",
    "## Head for comments should be the creation time of the first post in the dataset\n",
    "## Tail should be the last comment time of the last comment in a post in the dataset\n",
    "# Initial value: the date of creation of the last science post\n",
    "tail = sorted([x['created'] for x in science_posts.values()])[-1]\n",
    "comment_head = sorted([x['created'] for x in science_posts.values()])[0]\n",
    "for comment in all_comments:\n",
    "    if((comment['link_id'].replace(\"t3_\", \"\") in science_posts.keys()) and comment['created'] > tail):\n",
    "        tail = comment['created']\n",
    "print(\"{0} is the tail\".format(tail))\n",
    "print(\"{0} is the head\".format(comment_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recent_comments = [x for x in all_comments if x['created']>=comment_head and x['created']<= tail]\n",
    "print(\"Total number of comments: {0}\".format(len(all_comments)))\n",
    "print(\"Recent comments: {0}\".format(len(recent_comments)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (In this version, we do not fetch any missing posts, since we are only concerned with posts made within the analysis period. Any missing posts will be posts outside analysis period that received comments within the analysis period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# recent_comment_posts = list(set([x['link_id'] for x in recent_comments]))\n",
    "# unfetched_posts = []\n",
    "# for post in recent_comment_posts:\n",
    "#     if post.replace(\"t3_\", \"\") not in posts_2016.keys():\n",
    "#         unfetched_posts.append(post)\n",
    "\n",
    "# new_posts_fetched = 0\n",
    "# new_posts_saved = 0\n",
    "# for submission in r.get_submissions(unfetched_posts):\n",
    "#     new_posts_fetched += 1\n",
    "#     post = submission.json_dict\n",
    "#     post['created'] = utc.localize(datetime.datetime.utcfromtimestamp(int(post['created_utc'])))\n",
    "# #    we add all posts, so that we can fetch the covariates. But we omit them from the posts df\n",
    "# #    if(post['created'] > comment_head and post['created'] <= tail):\n",
    "        \n",
    "\n",
    "#     post['day.num'] = (post['created'] - utc.localize(datetime.datetime(1970,1,1))).days\n",
    "#     ## we do actually want to get all posts in this set so we can add their covariates\n",
    "#     if(post['banned_by'] is not None):\n",
    "#         post['visible'] = False\n",
    "#     else:\n",
    "#         post['visible'] = True\n",
    "#     if(post['created'] < tail and post['created'] > comment_head)\n",
    "#         posts_2016[post['id']] = post\n",
    "#         new_posts_saved ++ 1\n",
    "# print(\"Fetched {0} new posts\".format(new_posts_fetched))\n",
    "# print(\"Saved {0} new posts\".format(new_posts_saved))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch full post information for all posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_post_ids = [\"t3_\" + post_id for post_id in posts_2016.keys()]\n",
    "additions = 0\n",
    "for post_object in r.get_info(thing_id=all_post_ids):\n",
    "    post = post_object.json_dict\n",
    "    for key in post.keys():\n",
    "        if key not in posts_2016[post['id']].keys():\n",
    "            posts_2016[post['id']][key] = post[key]\n",
    "            additions += 1\n",
    "print(\"{0} fields updated\".format(additions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now attach post-level covariates to comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_ama(flair):\n",
    "    if(flair is None or (isinstance(flair, str))!=True):\n",
    "        return None\n",
    "    return \"ama\" in flair\n",
    "\n",
    "def is_badpost(flair):\n",
    "    if(flair is None or (isinstance(flair, str))!=True):\n",
    "        return None\n",
    "    return \"badpost\" in flair\n",
    "\n",
    "def strip_ama(flair):\n",
    "    if(flair is None or (isinstance(flair, str))!=True):\n",
    "        return None\n",
    "#    try:\n",
    "    flairs = [x for x in flair.split() if(\"ama\" not in x and \"badpost\" not in x)]\n",
    "#    except:\n",
    "#        import pdb;pdb.set_trace()\n",
    "    if(len(flairs)==0):\n",
    "        return None\n",
    "    return flairs[0] ## Confirmed that this is always only one flair\n",
    "\n",
    "recent_comments_included = []\n",
    "for comment in recent_comments:\n",
    "    post_id = comment['link_id'].replace(\"t3_\", \"\")\n",
    "    if(post_id in posts_2016.keys()):\n",
    "        post = posts_2016[comment['link_id'].replace(\"t3_\", \"\")]\n",
    "        comment['experiment.day'] = comment['created'].replace(hour = 10, minute=0, second=0)\n",
    "        comment['post.ama'] = is_ama(post['link_flair_css_class'])\n",
    "        comment['post.badpost'] = is_badpost(post['link_flair_css_class'])\n",
    "        comment['post.flair'] = strip_ama(post['link_flair_css_class'])\n",
    "        comment['post.created'] = post['created']\n",
    "        comment['minutes.since.post.created'] = (comment['created'] - post['created']).total_seconds() / 60.\n",
    "        comment['post.sub.top.minutes'] = post['post.sub.top.minutes']\n",
    "        comment['post.author'] = post['author']\n",
    "        comment['post.visible'] = post['visible']\n",
    "        comment['toplevel'] = comment['link_id'] == comment['parent_id']\n",
    "        comment['post.domain'] = post['domain']\n",
    "        comment['post.day.num']  = post['day.num']\n",
    "        comment['day.num'] = (comment['created'] - utc.localize(datetime.datetime(1970,1,1))).days\n",
    "        comment['weekday'] = comment['created'].weekday()\n",
    "        comment['weekend'] = (comment['weekday'] >=6)\n",
    "        recent_comments_included.append(comment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "recent_comment_df = pd.DataFrame(recent_comments_included)\n",
    "earliest_datetime = recent_comments[0]['created'].strftime(\"%Y.%m.%d_%H.%M.%S\")\n",
    "latest_datetime = recent_comments[-1]['created'].strftime(\"%Y.%m.%d_%H.%M.%S\")\n",
    "recent_comments_filename = \"r_science_comments_\" + earliest_datetime + \"-\" + latest_datetime + \".csv\"\n",
    "recent_comment_df.to_csv(os.path.join(\"outputs\",recent_comments_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"{0} comments by newcomers\".format(recent_comment_df[recent_comment_df['author.prev.comments']==0].created.count()))\n",
    "#print(\"{0} newcomer comments that were removed\")\n",
    "print(\"{0} comments by newcomers that were removed\".format(recent_comment_df[((recent_comment_df['author.prev.comments']==0) & (recent_comment_df.visible==False))].created.count()))\n",
    "print(\"{0} comments by newcomers that were removed, whose accounts were not later deleted\".format(recent_comment_df[((recent_comment_df['author.prev.comments']==0) & (recent_comment_df.visible==False) & (recent_comment_df.later_deleted==False))].created.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now Generate and output a post-level dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#recent_comments[0]['created']\n",
    "next_period_start = (recent_comments_included[0]['created'] + datetime.timedelta(days=1)).replace(hour = 10, minute=0, second=0)\n",
    "print(next_period_start)\n",
    "datetime.timedelta(days=1).total_seconds()\n",
    "#day_later - datetime.timedelta(hours = day_later.hour, minutes = day_later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from comment_head onward\n",
    "#post_comments = {}\n",
    "for post in posts_2016.values():\n",
    "#    post['newcomer.commenters'] = set()\n",
    "    post['newcomer.comments'] = 0\n",
    "    post['newcomer.comments.experiment.day'] = 0\n",
    "    post['newcomer.comments.experiment.after'] = 0\n",
    "\n",
    "    post['newcomer.comments.removed'] = 0\n",
    "    post['newcomer.comments.removed.experiment.day'] = 0\n",
    "    post['newcomer.comments.removed.experiment.after'] = 0\n",
    "\n",
    "    post['commenters'] = set()\n",
    "    \n",
    "    post['experiment.day'] = post['created'].replace(hour = 5, minute=0, second=0)\n",
    "    post['experiment.day.next'] = (post['experiment.day'] + datetime.timedelta(days=1))\n",
    "    post['experiment.day.minutes'] = int((post['experiment.day.next'] - post['created']).total_seconds() / 60.)\n",
    "                                   \n",
    "    post['num.comments.experiment.day'] = 0\n",
    "    post['num.comments.experiment.after'] = 0\n",
    "                                   \n",
    "    post['num.comments']  = 0\n",
    "    post['num.comments.removed'] = 0\n",
    "    post['num.comments.removed.experiment.day'] = 0\n",
    "    post['num.comments.removed.experiment.after'] = 0\n",
    "                                   \n",
    "    post['post.ama'] = is_ama(post['link_flair_css_class'])\n",
    "    post['post.badpost'] = is_badpost(post['link_flair_css_class'])\n",
    "    post['post.flair'] = strip_ama(post['link_flair_css_class'])\n",
    "    post['weekday'] = post['created'].weekday()\n",
    "    post['weekend'] = (post['weekday'] >=6)\n",
    "    \n",
    "for comment in recent_comments_included:\n",
    "    post = posts_2016[comment['link_id'].replace(\"t3_\", \"\")]\n",
    "    post['commenters'].add(comment['author'])\n",
    "    \n",
    "    \n",
    "    post['num.comments'] += 1\n",
    "    if(comment['created'] <= post['experiment.day.next']):\n",
    "        post['num.comments.experiment.day'] += 1 \n",
    "    else:\n",
    "        post['num.comments.experiment.after'] += 1\n",
    "    \n",
    "    if(comment['visible']!=True):\n",
    "        post['num.comments.removed'] +=1\n",
    "        if(comment['created'] <= post['experiment.day.next']):\n",
    "            post['num.comments.removed.experiment.day'] += 1\n",
    "        else:\n",
    "            post['num.comments.removed.experiment.after'] += 1            \n",
    "        \n",
    "    ## IF THE COMMENT AUTHOR IS A NEWCOMER\n",
    "    if comment['author.prev.comments'] == 0:\n",
    "        post['newcomer.comments'] += 1\n",
    "        \n",
    "        if(comment['created'] <= post['experiment.day.next']):\n",
    "            post['newcomer.comments.experiment.day'] += 1\n",
    "        else:\n",
    "            post['newcomer.comments.experiment.after'] += 1\n",
    "        \n",
    "        if(comment['visible']!=True):\n",
    "            post['newcomer.comments.removed'] += 1\n",
    "    \n",
    "            if(comment['created'] <= post['experiment.day.next']):\n",
    "                post['newcomer.comments.removed.experiment.day'] += 1\n",
    "            else:\n",
    "                post['newcomer.comments.removed.experiment.after'] += 1\n",
    "\n",
    "\n",
    "for post in posts_2016.values():\n",
    "    post['num.commenters'] = len(post['commenters'])\n",
    "    del post['commenters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"{0} posts overall\".format(len(list(posts_2016.values()))))\n",
    "print(\"{0} posts within the analysis period\".format(len([x for x in posts_2016.values() if (x['created'] > comment_head and x['created'] <= tail)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recent_post_df = pd.DataFrame([x for x in posts_2016.values() if (x['created'] > comment_head and x['created'] <= tail)])\n",
    "recent_posts_filename = \"r_science_posts_\" + earliest_datetime + \"-\" + latest_datetime + \".csv\"\n",
    "recent_post_df.to_csv(os.path.join(\"outputs\", recent_posts_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
